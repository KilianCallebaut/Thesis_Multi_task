% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Imoto et~al.(2020)Imoto, Tonami, Koizumi, Yasuda, Yamanishi, and
  Yamashita]{imoto2020sound}
K.~Imoto, N.~Tonami, Y.~Koizumi, M.~Yasuda, R.~Yamanishi, and Y.~Yamashita,
  ``Sound event detection by multitask learning of sound events and scenes with
  soft scene labels,'' in \emph{ICASSP 2020-2020 IEEE International Conference
  on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2020, pp. 621--625.

\bibitem[Caruana(1997)]{caruana1997multitask}
R.~Caruana, ``Multitask learning,'' \emph{Machine learning}, vol.~28, no.~1,
  pp. 41--75, 1997.

\bibitem[Oneto et~al.(2019)Oneto, Doninini, Elders, and
  Pontil]{oneto2019taking}
L.~Oneto, M.~Doninini, A.~Elders, and M.~Pontil, ``Taking advantage of
  multitask learning for fair classification,'' in \emph{Proceedings of the
  2019 AAAI/ACM Conference on AI, Ethics, and Society}, 2019, pp. 227--237.

\bibitem[Georgiev(2017)]{georgiev2017heterogeneous}
P.~Georgiev, ``Heterogeneous resource mobile sensing: computational offloading,
  scheduling and algorithm optimisation,'' 2017.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore,
  Plakal, and Ritter]{gemmeke2017audio}
J.~F. Gemmeke, D.~P. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C. Moore,
  M.~Plakal, and M.~Ritter, ``Audio set: An ontology and human-labeled dataset
  for audio events,'' in \emph{2017 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 776--780.

\bibitem[Duan et~al.(2014)Duan, Zhang, Roe, and Towsey]{duan2014survey}
S.~Duan, J.~Zhang, P.~Roe, and M.~Towsey, ``A survey of tagging techniques for
  music, speech and environmental sound,'' \emph{Artificial Intelligence
  Review}, vol.~42, no.~4, pp. 637--661, 2014.

\bibitem[BoreGowda(2018)]{boregowda2018environmental}
H.~BoreGowda, ``Environmental sound recognition: A survey,'' 2018.

\bibitem[Tagliasacchi et~al.(2020)Tagliasacchi, de~Chaumont~Quitry, and
  Roblek]{tagliasacchi2020multi}
M.~Tagliasacchi, F.~de~Chaumont~Quitry, and D.~Roblek, ``Multi-task adapters
  for on-device audio inference,'' \emph{IEEE Signal Processing Letters},
  vol.~27, pp. 630--634, 2020.

\bibitem[Tonami et~al.(2019)Tonami, Imoto, Niitsuma, Yamanishi, and
  Yamashita]{tonami2019joint}
N.~Tonami, K.~Imoto, M.~Niitsuma, R.~Yamanishi, and Y.~Yamashita, ``Joint
  analysis of acoustic events and scenes based on multitask learning,'' in
  \emph{2019 IEEE Workshop on Applications of Signal Processing to Audio and
  Acoustics (WASPAA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp.
  338--342.

\bibitem[Sakti et~al.(2016)Sakti, Kawanishi, Neubig, Yoshino, and
  Nakamura]{sakti2016deep}
S.~Sakti, S.~Kawanishi, G.~Neubig, K.~Yoshino, and S.~Nakamura, ``Deep
  bottleneck features and sound-dependent i-vectors for simultaneous
  recognition of speech and environmental sounds,'' in \emph{2016 IEEE Spoken
  Language Technology Workshop (SLT)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2016, pp. 35--42.

\bibitem[Fonseca et~al.(2017)Fonseca, Pons~Puig, Favory, Font~Corbera,
  Bogdanov, Ferraro, Oramas, Porter, and Serra]{fonseca2017freesound}
E.~Fonseca, J.~Pons~Puig, X.~Favory, F.~Font~Corbera, D.~Bogdanov, A.~Ferraro,
  S.~Oramas, A.~Porter, and X.~Serra, ``Freesound datasets: a platform for the
  creation of open audio datasets,'' in \emph{Hu X, Cunningham SJ, Turnbull D,
  Duan Z, editors. Proceedings of the 18th ISMIR Conference; 2017 oct 23-27;
  Suzhou, China.[Canada]: International Society for Music Information
  Retrieval; 2017. p. 486-93.}\hskip 1em plus 0.5em minus 0.4em\relax
  International Society for Music Information Retrieval (ISMIR), 2017.

\bibitem[Piczak(2015)]{piczak2015esc}
K.~J. Piczak, ``Esc: Dataset for environmental sound classification,'' in
  \emph{Proceedings of the 23rd ACM international conference on Multimedia},
  2015, pp. 1015--1018.

\bibitem[Paper and Paper(2021)]{paper2021tensorflow}
D.~Paper and D.~Paper, ``Tensorflow datasets,'' \emph{State-of-the-Art Deep
  Learning Models in TensorFlow: Modern Machine Learning in the Google Colab
  Ecosystem}, pp. 65--91, 2021.

\bibitem[Hannun et~al.(2014)Hannun, Case, Casper, Catanzaro, Diamos, Elsen,
  Prenger, Satheesh, Sengupta, Coates, et~al.]{hannun2014deep}
A.~Hannun, C.~Case, J.~Casper, B.~Catanzaro, G.~Diamos, E.~Elsen, R.~Prenger,
  S.~Satheesh, S.~Sengupta, A.~Coates \emph{et~al.}, ``Deep speech: Scaling up
  end-to-end speech recognition,'' \emph{arXiv preprint arXiv:1412.5567}, 2014.

\bibitem[Abe{\ss}er(2020)]{abesser2020review}
J.~Abe{\ss}er, ``A review of deep learning based methods for acoustic scene
  classification,'' \emph{Applied Sciences}, vol.~10, no.~6, 2020.

\bibitem[Huang et~al.(2019)Huang, Jia, and Guo]{huang2019state}
Z.~Huang, X.~Jia, and Y.~Guo, ``State-of-the-art model for music object
  recognition with deep learning,'' \emph{Applied Sciences}, vol.~9, no.~13, p.
  2645, 2019.

\bibitem[Park et~al.(2020)Park, Min, Bhattacharya, and
  Kawsar]{park2020augmenting}
C.~Park, C.~Min, S.~Bhattacharya, and F.~Kawsar, ``Augmenting conversational
  agents with ambient acoustic contexts,'' in \emph{22nd International
  Conference on Human-Computer Interaction with Mobile Devices and Services},
  2020, pp. 1--9.

\bibitem[Mitrovi{\'c} et~al.(2010)Mitrovi{\'c}, Zeppelzauer, and
  Breiteneder]{mitrovic2010features}
D.~Mitrovi{\'c}, M.~Zeppelzauer, and C.~Breiteneder, ``Features for
  content-based audio retrieval,'' in \emph{Advances in computers}.\hskip 1em
  plus 0.5em minus 0.4em\relax Elsevier, 2010, vol.~78, pp. 71--150.

\bibitem[Meyer(2019)]{meyer2019multi}
J.~Meyer, ``Multi-task and transfer learning in low-resource speech
  recognition,'' Ph.D. dissertation, The University of Arizona, 2019.

\bibitem[Lu et~al.(2004)Lu, Lu, Sehgal, Gupta, Du, Tham, Green, and
  Wan]{lu2004multitask}
Y.~Lu, F.~Lu, S.~Sehgal, S.~Gupta, J.~Du, C.~H. Tham, P.~Green, and V.~Wan,
  ``Multitask learning in connectionist speech recognition,'' in
  \emph{Proceedings of the Australian International Conference on Speech
  Science and Technology}.\hskip 1em plus 0.5em minus 0.4em\relax Citeseer,
  2004.

\bibitem[Ruder(2017)]{ruder2017overview}
S.~Ruder, ``An overview of multi-task learning in deep neural networks,''
  \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem[Zhang and Yang(2018)]{zhang2018overview}
Y.~Zhang and Q.~Yang, ``An overview of multi-task learning,'' \emph{National
  Science Review}, vol.~5, no.~1, pp. 30--43, 2018.

\bibitem[Lee et~al.(2019)Lee, Gong, Padhy, Rouditchenko, and
  Ndirango]{lee2019label}
T.~Lee, T.~Gong, S.~Padhy, A.~Rouditchenko, and A.~Ndirango, ``Label-efficient
  audio classification through multitask learning and self-supervision,''
  \emph{arXiv preprint arXiv:1910.12587}, 2019.

\bibitem[Ziaei et~al.(2013)Ziaei, Sangwan, and Hansen]{ziaei2013prof}
A.~Ziaei, A.~Sangwan, and J.~H. Hansen, ``Prof-life-log: Personal interaction
  analysis for naturalistic audio streams,'' in \emph{2013 IEEE International
  Conference on Acoustics, Speech and Signal Processing}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2013, pp. 7770--7774.

\bibitem[Xia et~al.(2019)Xia, Togneri, Sohel, Zhao, and Huang]{xia2019multi}
X.~Xia, R.~Togneri, F.~Sohel, Y.~Zhao, and D.~Huang, ``Multi-task learning for
  acoustic event detection using event and frame position information,''
  \emph{IEEE Transactions on Multimedia}, vol.~22, no.~3, pp. 569--578, 2019.

\bibitem[Seltzer and Droppo(2013)]{seltzer2013multi}
M.~L. Seltzer and J.~Droppo, ``Multi-task learning in deep neural networks for
  improved phoneme recognition,'' in \emph{2013 IEEE International Conference
  on Acoustics, Speech and Signal Processing}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2013, pp. 6965--6969.

\bibitem[Panchapagesan et~al.(2016)Panchapagesan, Sun, Khare, Matsoukas,
  Mandal, Hoffmeister, and Vitaladevuni]{panchapagesan2016multi}
S.~Panchapagesan, M.~Sun, A.~Khare, S.~Matsoukas, A.~Mandal, B.~Hoffmeister,
  and S.~Vitaladevuni, ``Multi-task learning and weighted cross-entropy for
  dnn-based keyword spotting.'' in \emph{Interspeech}, vol.~9, 2016, pp.
  760--764.

\bibitem[Georgiev et~al.(2017)Georgiev, Bhattacharya, Lane, and
  Mascolo]{georgiev2017low}
P.~Georgiev, S.~Bhattacharya, N.~D. Lane, and C.~Mascolo, ``Low-resource
  multi-task audio sensing for mobile and embedded devices via shared deep
  neural network representations,'' \emph{Proceedings of the ACM on
  Interactive, Mobile, Wearable and Ubiquitous Technologies}, vol.~1, no.~3,
  pp. 1--19, 2017.

\bibitem[Kim et~al.(2017)Kim, Lee, Ha, Lee, Lee, and Kim]{kim2017speech}
N.~K. Kim, J.~Lee, H.~K. Ha, G.~W. Lee, J.~H. Lee, and H.~K. Kim, ``Speech
  emotion recognition based on multi-task learning using a convolutional neural
  network,'' in \emph{2017 Asia-Pacific Signal and Information Processing
  Association Annual Summit and Conference (APSIPA ASC)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2017, pp. 704--707.

\bibitem[Nwe et~al.(2017)Nwe, Dat, and Ma]{nwe2017convolutional}
T.~L. Nwe, T.~H. Dat, and B.~Ma, ``Convolutional neural network with multi-task
  learning scheme for acoustic scene classification,'' in \emph{2017
  Asia-Pacific Signal and Information Processing Association Annual Summit and
  Conference (APSIPA ASC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017,
  pp. 1347--1350.

\bibitem[Phan et~al.(2017)Phan, Krawczyk-Becker, Gerkmann, and
  Mertins]{phan2017dnn}
H.~Phan, M.~Krawczyk-Becker, T.~Gerkmann, and A.~Mertins, ``Dnn and cnn with
  weighted and multi-task loss functions for audio event detection,''
  \emph{arXiv preprint arXiv:1708.03211}, 2017.

\bibitem[Sun et~al.(2017)Sun, Snyder, Gao, Nagaraja, Rodehorst, Panchapagesan,
  Strom, Matsoukas, and Vitaladevuni]{sun2017compressed}
M.~Sun, D.~Snyder, Y.~Gao, V.~K. Nagaraja, M.~Rodehorst, S.~Panchapagesan,
  N.~Strom, S.~Matsoukas, and S.~Vitaladevuni, ``Compressed time delay neural
  network for small-footprint keyword spotting.'' in \emph{Interspeech}, 2017,
  pp. 3607--3611.

\bibitem[Kremer et~al.(2018)Kremer, Borgholt, and
  Maal{\o}e]{kremer2018inductive}
J.~Kremer, L.~Borgholt, and L.~Maal{\o}e, ``On the inductive bias of
  word-character-level multi-task learning for speech recognition,''
  \emph{arXiv preprint arXiv:1812.02308}, 2018.

\bibitem[Morfi and Stowell(2018)]{morfi2018deep}
V.~Morfi and D.~Stowell, ``Deep learning for audio event detection and tagging
  on low-resource datasets,'' \emph{Applied Sciences}, vol.~8, no.~8, p. 1397,
  2018.

\bibitem[L{\'o}pez-Espejo et~al.(2019)L{\'o}pez-Espejo, Tan, and
  Jensen]{lopez2019keyword}
I.~L{\'o}pez-Espejo, Z.-H. Tan, and J.~Jensen, ``Keyword spotting for hearing
  assistive devices robust to external speakers,'' \emph{arXiv preprint
  arXiv:1906.09417}, 2019.

\bibitem[Pankajakshan et~al.(2019)Pankajakshan, Bear, and
  Benetos]{pankajakshan2019polyphonic}
A.~Pankajakshan, H.~L. Bear, and E.~Benetos, ``Polyphonic sound event and sound
  activity detection: A multi-task approach,'' in \emph{2019 IEEE Workshop on
  Applications of Signal Processing to Audio and Acoustics (WASPAA)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 323--327.

\bibitem[Phan et~al.(2019)Phan, Ch{\'e}n, Koch, Pham, McLoughlin, Mertins, and
  De~Vos]{phan2019unifying}
H.~Phan, O.~Y. Ch{\'e}n, P.~Koch, L.~Pham, I.~McLoughlin, A.~Mertins, and
  M.~De~Vos, ``Unifying isolated and overlapping audio event detection with
  multi-label multi-task convolutional recurrent neural networks,'' in
  \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2019, pp. 51--55.

\bibitem[Xu et~al.(2019)Xu, Huang, Cheng, and Song]{xu2019multi}
K.~Xu, S.~Huang, G.~Cheng, and X.~Song, ``A multi-task learning approach based
  on convolutional neural network for acoustic scene classification,'' in
  \emph{Proceedings of the 2019 2nd International Conference on Algorithms,
  Computing and Artificial Intelligence}, 2019, pp. 23--27.

\bibitem[Zeng et~al.(2019)Zeng, Mao, Peng, and Yi]{zeng2019spectrogram}
Y.~Zeng, H.~Mao, D.~Peng, and Z.~Yi, ``Spectrogram based multi-task audio
  classification,'' \emph{Multimedia Tools and Applications}, vol.~78, no.~3,
  pp. 3705--3722, 2019.

\bibitem[Abrol and Sharma(2020)]{abrol2020learning}
V.~Abrol and P.~Sharma, ``Learning hierarchy aware embedding from raw audio for
  acoustic scene classification,'' \emph{IEEE/ACM Transactions on Audio,
  Speech, and Language Processing}, vol.~28, pp. 1964--1973, 2020.

\bibitem[Deshmukh et~al.(2020)Deshmukh, Raj, and Singh]{deshmukh2020multi}
S.~Deshmukh, B.~Raj, and R.~Singh, ``Multi-task learning for interpretable
  weakly labelled sound event detection,'' \emph{arXiv preprint
  arXiv:2008.07085}, 2020.

\bibitem[Fernando et~al.(2020)Fernando, Sridharan, McLaren, Priyasad, Denman,
  and Fookes]{fernando2020temporarily}
T.~Fernando, S.~Sridharan, M.~McLaren, D.~Priyasad, S.~Denman, and C.~Fookes,
  ``Temporarily-aware context modeling using generative adversarial networks
  for speech activity detection,'' \emph{IEEE/ACM Transactions on Audio,
  Speech, and Language Processing}, vol.~28, pp. 1159--1169, 2020.

\bibitem[Huang et~al.(2020{\natexlab{a}})Huang, Lin, Ma, Wang, Liu, Qian, Liu,
  and Ouch]{huang2020guided}
Y.~Huang, L.~Lin, S.~Ma, X.~Wang, H.~Liu, Y.~Qian, M.~Liu, and K.~Ouch,
  ``Guided multi-branch learning systems for dcase 2020 task 4,'' \emph{arXiv
  preprint arXiv:2007.10638}, 2020.

\bibitem[Huang et~al.(2020{\natexlab{b}})Huang, Wang, Lin, Liu, and
  Qian]{huang2020multi}
Y.~Huang, X.~Wang, L.~Lin, H.~Liu, and Y.~Qian, ``Multi-branch learning for
  weakly-labeled sound event detection,'' in \emph{ICASSP 2020-2020 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 641--645.

\bibitem[Jung et~al.(2020)Jung, Shim, Kim, Kim, and Yu]{jung2020acoustic}
J.-w. Jung, H.-j. Shim, J.-h. Kim, S.-b. Kim, and H.-J. Yu, ``Acoustic scene
  classification using audio tagging,'' \emph{arXiv preprint arXiv:2003.09164},
  2020.

\bibitem[Komatsu et~al.(2020)Komatsu, Imoto, and Togami]{komatsu2020scene}
T.~Komatsu, K.~Imoto, and M.~Togami, ``Scene-dependent acoustic event detection
  with scene conditioning and fake-scene-conditioned loss,'' in \emph{ICASSP
  2020-2020 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  646--650.

\bibitem[Wu et~al.(2020)Wu, Jia, Nie, and Li]{wu2020domain}
H.~Wu, Y.~Jia, Y.~Nie, and M.~Li, ``Domain aware training for far-field
  small-footprint keyword spotting,'' \emph{arXiv preprint arXiv:2005.03633},
  2020.

\bibitem[Schmid(1997)]{schmid1997systematic}
H.~A. Schmid, ``Systematic framework design by generalization,''
  \emph{Communications of the ACM}, vol.~40, no.~10, pp. 48--51, 1997.

\bibitem[Roberts et~al.(1996)Roberts, Johnson, et~al.]{roberts1996evolving}
D.~Roberts, R.~Johnson \emph{et~al.}, ``Evolving frameworks: A pattern language
  for developing object-oriented frameworks,'' \emph{Pattern languages of
  program design}, vol.~3, pp. 471--486, 1996.

\bibitem[Ben-Abdallah et~al.(2004)Ben-Abdallah, Bouassida, Gargouri, and
  Hamadou]{ben2004uml}
H.~Ben-Abdallah, N.~Bouassida, F.~Gargouri, and A.~B. Hamadou, ``A uml based
  framework design method.'' \emph{J. Object Technol.}, vol.~3, no.~8, pp.
  97--120, 2004.

\bibitem[Bouassida et~al.(2001)Bouassida, Ben-Abdallah, and
  Gargouri]{bouassida2001uml}
N.~Bouassida, H.~Ben-Abdallah, and F.~Gargouri, ``A uml based design language
  for framework reuse,'' in \emph{OOIS 2001}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2001, pp. 211--221.

\bibitem[Dcase-Repo(2021)]{dcaserepo2021}
\BIBentryALTinterwordspacing
Dcase-Repo, ``Dcase\-repo/dcase\_util: A collection of utilities for detection
  and classification of acoustic scenes and events,'' 2021. [Online].
  Available: \url{https://github.com/DCASE-REPO/dcase_util}
\BIBentrySTDinterwordspacing

\bibitem[Dcase()]{dcase}
\BIBentryALTinterwordspacing
Dcase, ``Detection and classification of acoustic scenes and events.''
  [Online]. Available: \url{http://dcase.community/}
\BIBentrySTDinterwordspacing

\bibitem[Dong et~al.(2017)Dong, Supratak, Mai, Liu, Oehmichen, Yu, and
  Guo]{dong2017tensorlayer}
H.~Dong, A.~Supratak, L.~Mai, F.~Liu, A.~Oehmichen, S.~Yu, and Y.~Guo,
  ``Tensorlayer: a versatile library for efficient deep learning development,''
  in \emph{Proceedings of the 25th ACM international conference on Multimedia},
  2017, pp. 1201--1204.

\bibitem[Tokui et~al.(2019)Tokui, Okuta, Akiba, Niitani, Ogawa, Saito, Suzuki,
  Uenishi, Vogel, and Yamazaki~Vincent]{tokui2019chainer}
S.~Tokui, R.~Okuta, T.~Akiba, Y.~Niitani, T.~Ogawa, S.~Saito, S.~Suzuki,
  K.~Uenishi, B.~Vogel, and H.~Yamazaki~Vincent, ``Chainer: A deep learning
  framework for accelerating the research cycle,'' in \emph{Proceedings of the
  25th ACM SIGKDD International Conference on Knowledge Discovery \& Data
  Mining}, 2019, pp. 2002--2011.

\bibitem[Chen et~al.(2019)Chen, Cofer, Zhou, and Troyanskaya]{chen2019selene}
K.~M. Chen, E.~M. Cofer, J.~Zhou, and O.~G. Troyanskaya, ``Selene: a
  pytorch-based deep learning library for sequence data,'' \emph{Nature
  methods}, vol.~16, no.~4, pp. 315--318, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in neural
  information processing systems}, vol.~32, pp. 8026--8037, 2019.

\bibitem[ten()]{tensorflow}
\BIBentryALTinterwordspacing
Tensorflow—an open-source software library for machine intelligence.
  [Online]. Available: \url{https://www.tensorflow.org/}
\BIBentrySTDinterwordspacing

\bibitem[Nguyen et~al.(2019)Nguyen, Dlugolinsky, Bob{\'a}k, Tran, Garc{\'\i}a,
  Heredia, Mal{\'\i}k, and Hluch{\`y}]{nguyen2019machine}
G.~Nguyen, S.~Dlugolinsky, M.~Bob{\'a}k, V.~Tran, {\'A}.~L. Garc{\'\i}a,
  I.~Heredia, P.~Mal{\'\i}k, and L.~Hluch{\`y}, ``Machine learning and deep
  learning frameworks and libraries for large-scale data mining: a survey,''
  \emph{Artificial Intelligence Review}, vol.~52, no.~1, pp. 77--124, 2019.

\bibitem[De~Rainville et~al.(2012)De~Rainville, Fortin, Gardner, Parizeau, and
  Gagn{\'e}]{de2012deap}
F.-M. De~Rainville, F.-A. Fortin, M.-A. Gardner, M.~Parizeau, and C.~Gagn{\'e},
  ``Deap: A python framework for evolutionary algorithms,'' in
  \emph{Proceedings of the 14th annual conference companion on Genetic and
  evolutionary computation}, 2012, pp. 85--92.

\bibitem[pyt()]{pytorchmultitask}
\BIBentryALTinterwordspacing
pytorch\_multitask. [Online]. Available:
  \url{https://github.com/sugi-chan/pytorch_multitask/blob/master/pytorch%20multi-task-Copy2.ipynb}
\BIBentrySTDinterwordspacing

\bibitem[mul({\natexlab{a}})]{multiclasstutorial}
\BIBentryALTinterwordspacing
Csc321 tutorial 4: Multi-class classification with pytorch. [Online].
  Available: \url{https://www.cs.toronto.edu/~lczhang/321/tut/tut04.pdf}
\BIBentrySTDinterwordspacing

\bibitem[mul({\natexlab{b}})]{multidataset}
\BIBentryALTinterwordspacing
Unbalanced data loading for multi-task learning in pytorch. [Online].
  Available:
  \url{https://towardsdatascience.com/unbalanced-data-loading-for-multi-task-learning-in-pytorch-e030ad5033b}
\BIBentrySTDinterwordspacing

\bibitem[Wu et~al.(2015)Wu, Kinnunen, Evans, Yamagishi, Hanil{\c{c}}i,
  Sahidullah, and Sizov]{wu2015asvspoof}
Z.~Wu, T.~Kinnunen, N.~Evans, J.~Yamagishi, C.~Hanil{\c{c}}i, M.~Sahidullah,
  and A.~Sizov, ``Asvspoof 2015: the first automatic speaker verification
  spoofing and countermeasures challenge,'' in \emph{Sixteenth annual
  conference of the international speech communication association}, 2015.

\bibitem[Livingstone et~al.(2012)Livingstone, Peck, and
  Russo]{livingstone2012ravdess}
S.~R. Livingstone, K.~Peck, and F.~A. Russo, ``Ravdess: The ryerson
  audio-visual database of emotional speech and song,'' in \emph{Annual meeting
  of the canadian society for brain, behaviour and cognitive science}, 2012,
  pp. 205--211.

\bibitem[Mesaros et~al.(2017)Mesaros, Heittola, Diment, Elizalde, Shah,
  Vincent, Raj, and Virtanen]{mesaros2017dcase}
A.~Mesaros, T.~Heittola, A.~Diment, B.~Elizalde, A.~Shah, E.~Vincent, B.~Raj,
  and T.~Virtanen, ``Dcase 2017 challenge setup: Tasks, datasets and baseline
  system,'' in \emph{DCASE 2017-Workshop on Detection and Classification of
  Acoustic Scenes and Events}, 2017.

\bibitem[Pols et~al.(1977)]{pols1977spectral}
L.~C. Pols \emph{et~al.}, ``Spectral analysis and identification of dutch
  vowels in monosyllabic words,'' 1977.

\bibitem[Smith and Barnwell(1987)]{smith1987new}
M.~Smith and T.~Barnwell, ``A new filter bank theory for time-frequency
  representation,'' \emph{IEEE transactions on acoustics, speech, and signal
  processing}, vol.~35, no.~3, pp. 314--327, 1987.

\bibitem[Brown~Alan and Wallnau~Kurt(1996)]{brown1996framework}
W.~Brown~Alan and C.~Wallnau~Kurt, ``A framework for systematic evaluation of
  software technologies,'' \emph{IEEE Software, September}, 1996.

\end{thebibliography}
