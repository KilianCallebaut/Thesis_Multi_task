\chapter{System Design} \label{Design}

% Typing datasets
% Typing tasks
% Typing combining methods


\section{Framework Design} \label{Design:Framework}
%TODO: Explain how the system was modeled based on the design requirements
%TODO: Explain the concept of hotspots

Development frameworks have been defined by \citet{roberts1996evolving} as "reusable designs of all or part of a software system described by a set of abstract classes and the way instances of those classes collaborate". The bread and butter of framework design is developing abstractions that cut usual required work, while identifying the points where an implementing application likely requires to change. These variable points in an application domain are called hot-spots \cite{schmid1997systematic}. In the context of a framework, these are the points where the software allows to plug in application specific classes or subsystems. These hot-spots come in two forms: white-box and black-box hot-spots. White-box hot-spots require programming the plugged in blocks of code, while black-box hot-spots give the option to select from pre-implemented solutions. \\

Expanding existing ground work for deep learning systems with abstractions for developing and researching multi-task learning set-ups, means that the focus will be on providing abstractions cutting effort of combining datasets and tasks, but also that providing exhaustive possibilities are nearly impossible. Therefore, the design will only offer white-box solutions, but with a number of pre-made implementations addressing commonly available features in deep learning.\\

The nature of variability in the envisioned use is not static for a developed application. Researching multi-task set-ups requires implementing and executing multiple variations and comparing their results. The hot-spots thus have to be variable at run-time while ensuring the validity of the pipeline and differentiability of the different instantiations. Furthermore, in the interest of fast development, every variation in dataset handling must both be able to be implemented for one specific dataset or implemented for all at once.\\


%TODO: Class diagram is pretty detailed, include higher level explanation
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{"../../../../Documents/TU Delft/Thesis 2/System Explanation/CompleteSystemDiagram"}
	\caption[Complete System Class Diagram]{Class Diagram}
	\label{fig:completesystemdiagram}
\end{figure}

With these characteristics of variations in the framework in mind, the design UML is drawn up following the design principles from \citet{bouassida2001uml}. The definitions of white-box and black-box hot-spots differ slightly, in that they do not go for all descendants here. In stead black-box hot-spots cover classes with predefined code that can change their functionality based on allowed input variables for the class, yet should not be modified in code. The reason for this change is that in the original definitions complete class hierarchies where either white-box or black-box hot-spots where the classes and all of its inheritants either contained default code -and the desired implementation should be chosen from available options- or it did not. This is too rigid as it only allows extra extensions in a class if there is no default code.\\

In figure \ref{fig:completesystemdiagram} is the framework's extended UML class diagram. The explanation of the extra annotations goes as follows:

\begin{enumerate}
	\item Classes with a grey box in the top indicate that these classes  compose a white-box hot-spot. These classes require an extending implementation from the developer that can expand the original methods defined in the class.
	\item Classes with a black box in the top indicate that these classes are black-box hot-spots. These have default code present in their implementation and should not be modified.
	\item Grey circles in front of methods signify functions that change from one implementation to another. These are abstract methods that express variation points.
	\item Generalization relationships marked {incomplete} have base implementations that already have pre-made inheritants, but can be extended with extra classes.
	\item Highlighted borders signify that a class belongs to the program's core. The core are the 'frozen' parts of the system, which will remain unchanged in implementing applications made with the framework.
\end{enumerate}


Going into the model overview, without going into the details and inner workings, a structural explanation will be given. First thing to note is the central class TaskDatset, which is the encompassing object that standardizes the data and ensures its validity as input for the rest of the system. This is connected to possibly multiple Task objects which keeps task related information, separated from the dataset itself. Datasets can have multiple tasks and the same tasks can be found in multiple datasets. It also has an ExtractionMethod object which decouples all data transformation implementations. \\

On top of the TaskDatasets are two classes where it functions as a component in a composition structure. These are the HoldTaskDataset - which adds training set splitting and managing functionalities - and the ConcatTaskDataset - which adds the possibility to combine other datasets.\\

Alongside these, two creating classes can be found: the DataReader and the TrainingSetCreator. The DataReader has a grey square as it should be extended for every dataset implemented. This transitions a raw dataset to a standardized object. The TrainingSetCreator then take the different standardized objects, transforms and combines them into valid input for the training and evaluation functionalities.\\
 
This leads to the final section of the model, which is centered around the Training class. This in turn requires 4 classes to function: The ConcatTaskDataset for its input data, torch's nn Module for the Multi-Task model, a Results object for calculating and storing results and a TrainingUtils object for decoupling some of its methods and allowing their variation. While Task objects are contained in the TaskDataset, there is still a line drawn from the Training class, as these objects contain data that can change Task dependent functionalities in the Training class.\\

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../../../../Documents/TU Delft/Thesis 2/System Explanation/CompletePatternDiagram"}
	\caption{The framework's pattern diagram}
	\label{fig:completepatterndiagram}
\end{figure}

To further clarify the roles played by the classes in the network, a pattern diagram following the example in \citet{bouassida2001uml}. Two patterns - the composite patterns the ConcatTaskDataset and the HoldTaskDataset creates with the TaskDataset - are already mentioned above. The Task and the ExtractionMethod classes are also obvious strategy patterns as they decouple implementations from the original classes. The ExtractionMethod accumulates a few data transformations, which can be matched together through its implementing Decorator pattern. \\

How these classes function and why these methodologies were chosen will be explained further in the section, but two classes should be still highlighted in the overview. One is the TaskDataset which also has a builder role. This includes adding partmental creator functionalities, which are used here to ensure that a TaskDataset is correctly implemented as well as facilitating the process of building such complex object. The other is the Pipeline pattern of the TrainingSetCreator. This creates a sequence of operations that lead from the raw datasets to combined inputs for a specific training set-up. The Pipeline pattern is what allows every step up to training to be scalable, modularly variable and correctly executed. \\

One guiding design rule for this framework was that no class should assume anything about the underlying implementation of another. This is to ensure modularity in the system. Given that the framework is a white-box implementation, most developmental speed-up is made by letting the developer only focus on one class per responsibility and giving the ability to execute the same parallel operations as if they were performed on one object. The overall design envisions the whole pipeline from datasets to trained models, but these can be picked apart according to the developer's needs. A TaskDataset will always be valid input for training and evaluation, no matter if it came from a DataReader. A Results object can always write data to and read data from files, no matter if it happens in training. \\


\section{Assumptions} \label{Design:Assumptions}

Before resuming to the detailed explanations of the operation designs, a small summary of the design assumptions made is given:

\begin{itemize}
	\item The device is unlikely to hold many datasets in memory at once, yet for systems that can, this might still be desirable for the speed-up it can provide. 
	\item The model creation is central and developers should be able to build pytorch models unrestrained. 
	\item The training and evaluation loops should not required modification, as they are static for all (gradient descent based) applications, but they should take variable functionalities.
	\item Hot spots for variation will likely have to be varied at each point within the runtime itself
	\item Preprocessing will have to be both easily made the same for multiple datasets as have an individual process for each one
	\item Introducing more execution time for creating and preparing the data for training and evaluation in the name of flexibility and added features is more acceptable than it is for executing the actual training and evaluation. Training and execution take significantly longer and can grow exponentially based on the dataset size and amount of epochs, while data preparation only has to be performed once before each run.
	\item As soon as the implementation is made, the developer wants to walk away until the results are in. The developer must both be able to define the pipeline as the variations they want to investigate beforehand, while the system must offer checkpoints to restart in case a malfunction happened during execution.
	\item Mistakes will be made.
\end{itemize}


\section{Key Decisions}

%TODO: What are the key decisions that were made and why?
% - Data Reader is a abstract extendible class
% - Manipulations are hooks on the TaskDataset object itself
% - The TrainingSetCreator is the way to assemble the pipeline
% -- Transformations are called by name
% -- training and eval is not included ==> because the option is left open for model definition to give as much space for the model as possible
% -- training and evaluation mechanisms are functions with optional object inputs which can modify or add functionality
% -- The test set is a taskdataset in the Holdtaskdatset, while the training set is just contained in the HoldTaskDataset itself
% -- Extraction method and instance transformations are contained within the same object which is a decorator
% -- The tasks are defined through their own objects

Starting off, in order to give a clearer understanding of the ideas behind the design, a summary is made of the key design decisions and their reasoning behind this. From these ideas, the rest of the framework is filled out and constructed. In essence, the goal of what is supposed to be built - the pipeline - can be found illustrated in figure \ref{fig:simplifiedoverview}. To achieve this, the key structural options that were taken are as follows:

\begin{enumerate}
	\item Datasets and the way they are stored have an infinite amount of variations all of which can't be anticipated. The situation is to make sure that a developer is not limited by the systems finite amount of data structure assumptions, while still ensuring that they create an object which will have no issue in the rest of the system, as well as offer developmental time cutting tools in the extraction process. As a response, the framework in this stage was designed with an abstract DataReader class which the developer has to extend for their particular dataset, that gradually fills a TaskDataset which is the standardized object to encapsulate the data. The DataReader contains abstract functions that the developer has to implement, for which it has a predefined execution order. Ensuring validity happens through builder methods which verify the correctness of the input and the calling of the validation function in the DataReader that checks that the final object is correct.
	\item For reasons of modularity, the choice was made to have all dataset manipulations be defined in the TaskDataset class itself and not some external class. This way, no external class has to make assumptions on the exact structure of the inputs and targets which makes it more feasible to create an extension to if required. These manipulations often work in relation with the ExtractionMethod object, which is a class that is responsible for the modification of singular feature matrix instances. This thus makes the functions on the TaskDataset responsible for navigating the structure and the ExtractionMethod responsible for changing the content of the structure.
	\item The pipeline up until the initiation of training and/or evaluation is assembled through using the TrainingSetCreator. This consists of adding DataReaders, ExtractionMethod objects, filtering definitions, etc in a builder like pattern, which is consequently executed by calling its execution function. In other words the developer can add elements as they desire and consequently assemble the process the way they want after the fact. Adding elements to the TrainingSetCreator can be scaled to all involved datasets (if desired) this way and also be replaced after the fact without having to redefine the entire pipeline. 
	\item Tasks and datasets are two different things and so tasks have become their own objects which can be present in multiple TaskDatasets. These objects contain all general task related data (e.g. the exact label names) as well as methods for handling their related outputs. The situation when tasks are present in multiple datasets is streamlined in the ConcatTaskDataset, where their related target labels are placed at the same positions.
	\item The training and evaluation mechanisms are pretty singular functions, with some internal behaviour that can be redefined as it redirects them to an external object. This definition is pretty rigid as a way to reliably separate the complicated numerous combinatory possibilities into individual task handlings. Another thing is that, while performance was not a main objective in this work, it was to be avoided to introduce possibly too many extra redirections and overhead by completely disassembling its functionalities for the sake of flexibility. 
	\item Building on the previous point, as discussed in \cite{roberts1996evolving}, more classes introduce more complexity, and so it was avoided to introduce too many specialized case handlings, unless when they were made extendible through a hot-spot. This explains more why instances like the training mechanism are structured the way they are - since they are essentially functions and object oriented definitions would assumably do more harm then good - as well as why manipulations are defined on the TaskDataset objects themselves and not through an external class.
\end{enumerate}

\section{Creating standardized objects} \label{Design:StandardizedObjects}

One of the most important aspects of this framework is the addition of standardized objects to encapsulate the input. These allow for easy manipulation and combination of different datasets. The objective is that once created, the developer should not have to worry about them further down the pipeline. \\


The standardized object is called the TaskDataset. This contains the feature matrices extracted from the audio files in the dataset, their target labels which form the ground truths and their corresponding task information. Creating these objects from the raw datasets is a complex task which requires the navigation of their storage structure, that is specific for each dataset. Developers should thus be responsible for getting the correct data out of the dataset but should get proper assurance the object was created correctly. \\

The design choice was thus made to create an abstract factory the developer has to extend, loaded with the necessary tools to insert the data correctly. In order to provide the necessary assurances the object is created correctly as well as facilitate the process, the TaskDataset is loaded with builder functions. Through these the TaskDataset can be filled step-by-step. This further achieves two things. One, because the functions are in the TaskDataset itself, \textit{no outside classes have to make any assumptions about internal data structure or workings of the TaskDataset}. This ensures modularity and extendibility of the TaskDataset and will be a constraint held for the rest of the design. Two, because datasets can be too large to be loaded in the system's working memory, this way each instance can be inserted one by one in the data object. \\

Back to the abstract factory - the DataReader - where the abstract methods are called in a predefined pattern, so the developer only has to extend the required methods and adhere to their required responsibilities. To aid with extraction and processing the sound files from different datasets in the same way, this class also has a processing function for the numerical time series representing the audio. Included here are abilities like resample audio to the same rate and converting multi-channel audio to a single channel. The developer themselves can choose to use this or not. Along with creating the standardized object, the factory also includes quick saving and reading of the created object for the specific method of extraction. This makes it more feasible to quickly reload the dataset without having to redo the extraction. \\

The structure for storing data in the TaskDataset itself then is as follows. Since in audio every instance likely has a different size due to varying audio lengths, the collection of input matrices are stored as tensors in a Python list. Each target then is stored as a list of integers. Usually they are encoded as binary strings with a 1 in each column number that an instance has a corresponding label for. The target labels in the order which relates to the column numbers and the rest of the task information is recorded in a Task object which is stored in the TaskDataset as well. \\

\subsection{Variations}


The default assumption is that a dataset has a single list of unrelated inputs with a target label for each instance. This section will explain how variations on this can also be contained in the TaskDataset. First variation are datasets which have separated predefined training and test sets. These will have to be combined with datasets that have to be split afterwards. Both training and test dataset have to receive the same preprocessing, so to enforce this their objects have to be somehow linked. This is done through making the a composite object that is a TaskDataset which contains a test TaskDataset. The test data can then be stored in this test object inside the composite object, while sharing the same preprocessing functions as well as appropriately handling called functions on it to allow it to be treated uniformly to a dataset without presplit data. \\

Next variation are datasets that have target data for multiple tasks. While one task with according targets is required, the rest can be added in a list of tuples of task objects and a list of targets same as for the 'main' task. \\

The last one are datasets that have "groupings". What is meant here are datasets consisting of audio files that are already split, with each part having their own ground truth, but cannot be divided later (e.g. parts of the same split going to the training set and the test set). For this instance, the same formality is used as in sklearn's splitting functions and a grouping list can be stored where the index corresponding to each instance belonging together contains the same number. \\

The objective is to treat the resulting TaskDataset uniformly regardless of variation. The strategy for handling each of these cases will be discussed in the appropriate sections later. \\


\section{Manipulating datasets} \label{Design:Manipulating}

As mentioned before the created standardized objects are also responsible for providing easy manipulation abilities for datasets. The choice was made to include dataset changing handles on the object itself, which can be called by other classes, without having them know the underlying representation of the object itself. These handles fall into two categories: functions that transform data instances and functions that change the collection itself. \\

The functions that transform instances are the most likely to change, as these have to be tweaked based on their effect on performance of the resulting recognition system. What we understand under these are data scaling methods, windowing functions, but also feature extraction as this transforms the original time series representation to one fit for the required learning task. The data scaling and other possibly required preparation functions will depend on the feature extraction method used. For example, a feature extraction method which outputs a time dependent representation will require a windowing function that cuts the feature matrices to the same shape in order to fit them in the same batches, while one that outputs matrices of the same shape will not. To adress this along with the likelihood of change, the instance transformation functions are encapsulated in an object called the Extraction\_Method. \\

This is an abstract template that developers have to extend in order to define their required implementation. Parameters for these implementations can be given on the fly as input to instantiate these objects. The whole object is then stored in return as input in the TaskDataset which will then use it when the call is made to transform the data. The extraction\_method object has a decorator inheritant, with a number of premade classes that already implement numerous transformation functions, as can be seen in figure \ref{fig:completesystemdiagram}.\\

The extracion\_method object accumulates a number of transformations that depend on which extraction method is used. These can be categorized as feature extractors, scalers, preparation fitters and preparation executors. Feature extractors create a feature matrix tensor from a given time series representing an audio file. Scalers include calculating metrics for scaling and then using those to execute the scaling of the data. Preparation fitters and executors are separated. Executors mainly concise of framing and segmenting operations which transform the data in matrices of the same size. Fitters calculate the parameters for these operations, but these operations can always be executed with parameters given by the user.\\

The Extraction\_Method object has another hidden function which lies in its name variable. The TaskDataset uses this object's name to store its extracted feature matrices to files. This happens in the interest of being able to quickly reiterate and compare different variations. The extraction method itself also gets stored, to allow for recreating the input at a different time than the original extraction and transformations were done.\\

Functions that change the collection itself then are ones that filter and split the data. For these ones it does not matter what is contained in the actual instances, but simply how the collection is composed. Filtering the data instances - based on their associated labels - is present to offer the ability of adjusting the label distribution. This happens through taking a (pseudo)random sample of data instances with the associated label of the defined size the developer wants the (maximum) amount of labels to be. The rest then gets filtered out of the current dataset object. \\

Splitting the dataset then in a train and test set requires a more sophisticated strategy. Because both require that the same transformations are performed, with some additional constraints (see further), it is opted for creating a composite object which both is a TaskDataset and contains a TaskDataset that is the test set. Splitting the dataset into a train and test set is performed then by filling this test set with data split from the original, while both share the same object - with the same parameters - for performing transformations. As mentioned before, there are datasets which have test sets defined beforehand. The difference between these two situations is thus that the developer fills this test dataset beforehand. At the point where a dataset has a filled test set, both of these situations thus become the same again for the rest of the process. \\

Creating training and test sets themselves do not simply happen at random either. The default case is that these get a stratified collection, meaning that the resulting folds will try to stick as close to the original label distribution as possible. Mentioning folds gives away that the splitting is not simply meant for one time use. The splitting into train and test sets actually contains three operations. First is splitting the data into k equally sized folds based on their indices. Second takes indices and transforms them into a train and test TaskDataset, while returning previous data to the original set. Third simply handles the difference between predefined sets and sets where the splits still needs to happen, calling the previous functions in the latter case. \\



\section{Combining Datasets} \label{Design:Combining}

When the datasets are created and prepared for training, they need to be combined as a single dataset, but from which the instances can be processed differently per task in training and evaluation. The solution for this is another composite object called the ConcatTaskDataset. Built on the groundwork from PyTorch's ConcatDataset, this class contains a list of TaskDatasets and builds a front for the data loading in training making it seem like one big dataset. Alongside this, the class contains methods for differentiating the different tasks afterwards. At instantiation every individual dataset gets loaded with information to combine the target vectors into one. More detailed information on the exact problems that need to be addressed and how it achieves this are given in section \ref{Impl:DataLoad}. \\

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../../../../Documents/TU Delft/Thesis 2/System Explanation/SequenceDiagram"}
	\caption{Sequence diagram illustrating how the TrainingSetCreator extracts the data from datasets to training and test sets}
	\label{fig:sequencediagram}
\end{figure}

To illustrate the execution flow of creating the input for training and evaluation a sequence diagram is given in figure \ref{fig:sequencediagram}. In the above sections the designs are detailed for the individual handling of the datasets. This diagram illustrates how the TrainingSetCreator brings the operations together by functioning the handles present on the above classes. As mentioned in \ref{Design:Framework}, this class is a pipeline pattern where dataset and manipulations can be optionally added and scaled to individual or all TaskDatasets at the same time.\\

Not only does this add modularly replaceable sections in the handling of data, the TaskDataset objects are reset every time a new pipeline piece gets added. This allows modifying the creation of a TaskDataset while keeping the ones that do not need to be altered. Imagine the situation where multiple TaskDatasets are created and combined, when the developer wants to alter the creation of one of them. In stead of having to recreate all the objects, only one of them is reset and remade. \\


\section{Creating Model} \label{Design:Model}
%TODO: Create visual representation of dev workflow
This framework is aimed at providing a facilitating extension to PyTorch for training and testing multi-head hard parameter sharing models. Model creation thus puts as little requirements on designing the actual models as possible. The rest of the system builds on Pytorch's classes as to ensure that the input would be valid for any PyTorch module. The only presumption the system has to make is that the output is a tuple with each entry being the predicted results for each task in the order of the task list from the input dataset. While no class was made that ensures the developer does this correctly - with the model creation being central, the developer should have the freedom to create whatever they want, adjusting the functionalities around it if necessary - there are two base multi-head networks, a CNN and a DNN, available which has variable layers based on the input. These also already include adaptations based on the type of task. In this area the mantra is that the developer knows best. \\


\section{Training and Evaluating Model} \label{Design:Training}

When the model is designed and the input dataset is created, they will be inserted into the method which trains and/or evaluates a model. These two are similar, with the only exception that the training function updates the model parameters every loop. Training (and evaluating) a model happens through a method which should remain static, but can vary its functionalities based on input. The reason for this is that the framework can only ensure the validity of the created pipeline if it controls how inputs are received and outputs are processed. In other words if it knows what it is going to do with the received data. \\

The handling of objects and data which the rest of the system does not rely on however, can be overwritten in numerous ways, depending on the required change. These are all discussed in section \ref{Impl:Extendibility}. Every batch statistics are calculated from comparing the model's results to the ground truths. This happens through an adapter class that takes the predicted results and is responsible for writing them to the correct files. This class, the Results object, is wired to write results already to TensorBoard. This library provides easy visualization for deep learning metrics. This happens live, both in the training and evaluation loop, so that developer can follow the progress and intervene during the loop if necessary. The developer can easily extend this class and define their own desired metric calculations, as the results only receive the ground truths, predictions and the loss. \\

Alongside metric calculation and storage, the Results object also provides checkpointing function for the model's parameters. This makes it possible to retrieve a model's previous states. The Results object thus acts as a unique adapter for each training run, managing and distributing files resulting from training and evaluation. Because of this design choice, it is possible to continue or restart previous runs by instantiating the same Results object and giving it to the "blind" training and evaluation functions. \\

Especially the evaluation function dynamically uses this, as it can either take a model object or load up the model from each epoch if none was given. This simple aspect allows a developer to use the system as a testing framework for trained created models if they need to. \\

Changing non-metric related functions - like early stopping - can be done by another extra object which can be given as input, namely the Training\_Utils object. This simply contains functions, for which the developer can write an extending object and give as input. \\ 

\section{Comparing the steps} \label{Design:Comparing}

In order to truly grasp what the design is offering in terms of developmental aid, a comparison is made to development without the framework. Considering that there is a lack of a comparative framework for building multi-task pipelines as well as to get working examples of multi-task implementations, three demonstrating implementations are taken \cite{pytorchmultitask} \cite{multiclasstutorial} \cite{multidataset}. These are tutorials on multi-task implementations in PyTorch which give a good sense on how research oriented implementations -which focus on simply getting the results and not developing a good maintainable system- are made. While these sorts of implementations will mainly be illustrative for implementations by more inexperienced developers, they are a good demonstration for the sorts of problems that arise by the lack of a framework which can be present to smaller degrees even in implementations by more experienced people.\\

A comparison of the involved steps is drawn up:\\

\fbox{
	\begin{minipage}{\linewidth}
		
	{\large\textbf{ Development steps without the framework}}
	\begin{enumerate}
		\item Implement an extension to the PyTorch Dataset
		\begin{itemize}
			\item Either build a new extending class for each dataset or build a dataset structure which can be reutilized for all involved datasets
			\item Define the data encapsulating structures (e.g. list of tensors for the input feature matrices)
			\item Define the getter function, including what it returns which will be used later when iterating over the data, as well as which outputs will be batched together
			\item Define the length function
		\end{itemize}
		\item Iterate over the raw dataset and extract the inputs and targets to the PyTorch Dataset extension(s)
		\item Implementation and execution of the input transformations
		\begin{itemize}
			\item Either the transformation parameters are set in stone or they have to be calculated from the dataset
		\end{itemize}
		\item Implementation and execution of the input filtering functions
		\item Creation of training and test set
		\item Definition and implementation of the training and evaluation loops
		\begin{itemize}
			\item Multi-task loss calculation means separate calculations and subsequent combining of them
		\end{itemize}
		\item Output evaluation metric calculation and storage
		\item Model creation
		\item Execution of the training and evaluation mechanisms with the desired parameters
		\item Implement and execute the visualization of the output evaluation metrics
	\end{enumerate}
\end{minipage}}
\bigskip
\bigskip

\fbox{
	\begin{minipage}{\linewidth}
	
	{\large\textbf{ Development steps with the framework}}
	\begin{enumerate}
		\item Extend the DataReader
		\begin{enumerate}
			\item Implement the data gathering/indexing function
			\item Implement the input insertion
			\item Implement the insertion of the targets AND the task
	%		\item Optionally implement different modes of extracting the data based on inserted parameters
		\end{enumerate}
		\item Assemble the pipeline through the TrainingSetCreator:
		\begin{enumerate}
			\item Assemble and add the ExtractionMethod
			\item Add parameters for filtering/sampling the data 
			\item Add normalization call
			\item Add transformation call
		\end{enumerate}
		\item Execute the pipeline
		\begin{itemize}
			\item Either return complete prepared datasets
			\item Or a generation function for k-fold train/test generation
		\end{itemize}
		\item Model creation
		\item Instantiate Result object
		\item Insert data, model and Result object into train and evaluation mechanisms
		\item Investigate and compare the results in TensorBoard
	\end{enumerate}

\end{minipage}}


\bigskip
\bigskip


First, the easiest comparison to make is in terms of which steps exactly are being taken care of. Starting from the top, an extension has to be made in both cases for extracting the input. The difference is that without the framework there has to be an extension to the PyTorch dataset as well as subsequently iterating over and extracting workable data, while the framework only requires the iteration and then gradually the valid object can be built. Then, the dataset manipulations like transformations and sampling always have to be defined, implemented and executed. Compare that to the frameworks methodology using the ExtractionMethod class which can be assembled with premade function definitions how the developer wants and transformation executions become simple 1 line calls. Sampling is also readily available with parameters that define its functioning. Train and test set creation get reduced to a simple line call cutting its need to define. Implementation and evaluation loops no longer need to be defined, nor do the metric calculation and visualisation, both cases being reduced to simple 1 line calls to function. The model creation is still present and as much workload for both cases.\\

While it is easy to point out how much the framework cuts in code compared to implementations which were only made for a single function, its inclusion is mainly meant to demonstrate how even these single function implementations can still be performed faster and clearer through the framework. However, there are two more subtle developmental problems which are targeted in this design. What these implementations do is performing pre-designed methodologies, which does not actually reflect how development works, meaning there is a hidden undercurrent of steps that can not be seen by laying the implementation steps side by side like this.\\ 

The two extra aspects that are examined are modifications and expansions. In order to understand the modifications work load - how they arise in a development process, how a multi-task, multi-dataset setting can exacerbate the problem - consider why modifications are required and how these rigid systems would force them to be handled. Modifications are often required in order to get the implementation even working. In case the transformation function needs to change in \cite{pytorchmultitask} and \cite{multiclasstutorial}, because an error is given (e.g. the input matrix is resized to an incompatible size) the developer has to replace the code in multiple places and again execute the whole extraction process as well. If then there is still a fault which doesn't raise an error, but leads to invalid results, this has to be redone again. After that if there is no more error, but the results aren't optimal wherefor the transformation is varied to be optimized, the same thing happens multiple times and everything keeps getting recalculated from the raw datasets. Consider then that this has to be done for 1) multiple datasets 2) multiple elements (e.g. the extraction method) 3) multiple locations of their calls. Simply put, modification is an inseparable part of implementation as well as the whole development cycle and if it isn't anticipated, the high, error-prone, repetitive workload ensues. The framework reduces these type of calls to singular, extendible places as to avoid having to keep track of where to make modifications. An example of this is the HoldTaskDataset structure which automatically handles the correct redirection of the functionalities to the training set and test set where needed. Furthermore, when the interest lies in modifying and keeping track of the best results, using the framework offers the feature of replacing a part at runtime through the pipeline structure in the TrainingSetCreator. \\  

The other aspect are expansions. One of the ideas of introducing this framework is to stimulate research, which in this regard means avoiding issues for experimentation. Extra steps, datasets, parameters and/or functions are insertable while the other parts of the pipeline are designed for flexibility. Looking at \cite{pytorchmultitask} for example, each input is linked to a set amount of different task labels. This means that the training module only works for that specific dataset, perhaps combined with others as long as they have the exact same structure. This framework is there to open the doors for unspecified amount of combinations of tasks and datasets. Not only experimentation is possible through this, but the gradual building of more complex pipeline structures for performance optimization as well. The key here in design is the flexibility of its structures and the extensibility to define new ways of handling the objects. Another important aspect is in its metric calculations and visualizations. The Results object offers both the option to have no limit on what a developer can add in terms of visualizations, but also includes the power to load old models and expand their visualizations.\\

To reiterate, the referred examples are not particularly well written implementations, but do offer the opportunity to clearly indicate how the framework can simplify the process of implementing a multi-task system as well as quickly open it up to new opportunities. Aside from time saving shortcuts to implement one solution to a multi-task classification problem, the system also thinks about shortcuts in the optimization process as well as the experimentation process.\\


\section{Three Implementations} \label{Design:Implementations}

With the base strategies in the pipeline designed, it might seem complex without a clear reason why its abstractions were built. This section will give insight to how the framework was developed alongside giving clear examples of how the previous designs are instantiated. This is not meant to be taken in a way where the design followed from separate rudimentary implementations of this problem, but how the design was incrementally refined to serve more problems.\\

The designing process for a framework is broken down by \citet{roberts1996evolving} as follows. Frameworks are reusable software patterns that facilitate the development of applications. Determining the correct abstractions must come from concrete examples, as it is nearly impossible to have to foresight to address the required functionalities from simply the domain. A number of abstractions do not become apparent until the framework has been reused. Generalizable solutions can only come from actually building the applications. \\

What \citet{roberts1996evolving} propose as a simple step by step plan is to build three applications in the same problem domain which differ from each other each time. While the more applications get developed lead to a more generalizable framework, there has to be a cut off point as too many applications can make it impossible to actually finish the work. That being said, a framework is likely to continue to evolve after the three applications are made. What follows now is an explanation of the three applications that shaped the design of the framework, ending with conclusions drawn from them and further extensions. \\

{\large \textbf{Choice}} \\

First however, the choice of projects must be decided. Each of them will be acoustic multi-task classifiers with different requirements for the system. The focus for this platform is research and iteratively designing the best performing multi-task system. Therefore, the first two implementations are following two research papers. This choice also adds an opportunity to evaluate the system by comparing to the reported results. Generalizing for multi-task purposes means that the choices must cover enough datasets, tasks and data processing features. Implementing existing research can be helpful here for discovering possible set-ups, one must take in account that this is completed work. The process of discovery and improvement of systems is not usually covered in the resulting paper. Therefore, the last implementation actually went into trying to develop new functioning set-up for multi-task research . \\


%Explain Georgiev

{\large \textbf{Low-resource Multi-task Audio Sensing for Mobile and Embedded Devices via Shared Deep Neural Network Representations}} \\

In the work done by \citet{georgiev2017low}, a Deep Neural Network is developed which tries to create a general purpose audio task model that addresses computational limitations of mobile, embedded and IOT devices. The approach is to bring 4 separate audio tasks together in one deep learning framework, for which they try out different configurations. Every task is tested combined in a multi-head network as well as separated after which they evaluate the effect on performance for each of them. The model that combined each in a single multi-head model was not significantly worse than the best performing one, making the multi-task set-up  a viable way to reduce resource requirements. \\

The chosen tasks here were Speaker Identification, Emotion Recognition, Stress Detection and Acoustic Scene Recognition. These are three background identification tasks, meaning they don't actually require to know what happens in an audio fragment and can take a longer time frame for labeling. To rebuild the actual set-up this work utilised the ASVSpoof database \cite{wu2015asvspoof} for the Speaker Identification task, the Ravdess database \cite{livingstone2012ravdess} for emotional speech recognition task and the DCASE 2017 Acoustic Scene dataset \cite{mesaros2017dcase}. The stress detection dataset was a subset of the Ravdess dataset with different audio lengths. \\

For extracting features they utilised both MFCC \cite{pols1977spectral} and their own created summary of filter banks. This summary consists of creating different statistical aggregation metrics (e.g. the mean, the standard deviation, the median, ...) per coefficient from extracted log filter banks \cite{smith1987new}. This creates a representation from an audio sample independent of time and requiring less space.\\


\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../../../../Documents/TU Delft/Thesis 2/System Explanation/GeorgievModel"}
	\caption{The Multi-Task pipeline and Model used in \citet{georgiev2017low}}
	\label{fig:georgievmodel}
\end{figure}

For design purposes, the multi-task pipeline is examined. The system is built for low quality audio with a sample rate of 8 kHZ. From the audio MFCC or the logbank summaries are extracted. Each task has a different dataset but all the inputs require the same extraction. The resulting matrices are then normalized using the mean and the standard deviation from the numerical data. This is then used as input for training the model, which is done using gradient descent. The data is fed to the training module in stratified batches. In this context, it means that a batch contains samples from every dataset, with the amount of samples from every dataset matching internal size ratios of the datasets. The summary of this can be found in figure \ref{fig:georgievmodel} \\

A few lessons have been made from recreating this exact set-up. 

\begin{itemize}
	\item Every audio sample should be able to be resampled which will likely happen at all the datasets at once
	\item Feature extraction as well as any other preprocessing will have to be easily replicable for every dataset at once
	\item Features can be time related or not. If the dataset has varying time lengths then the resulting feature matrices will be of varying lengths. These can not be batched together for data loading. The solution for this is to include an operation that can transform feature matrices to the same size in a windowing function. 
	\item Depending on the desired batching method, it is possible that data instances from all datasets and tasks have to go in the same batch. This does not only require that input instances have to have the same size, but also that target vectors can go in a unified matrix structure.
	\item Normalization using the mean and the standard deviation happens differently for these two feature extraction methods. In MFCC, the mean and standard deviation is calculated per one dimension in the feature matrix: the coefficients. This makes sense as the second dimension are time steps, so the numerical distribution will be from the same domain. However for the statistical summary matrices, every value in the second dimension is a different metric. Mean and standard deviation calculation must happen in this instance per cell basis. In essence for the system, this means that both these scaling methods need to be covered, but also that normalization (and possibly any other transformation operation like windowing) depend on the extraction method used.
	\item The DCASE dataset has a predefined train and test set. The other two do not. The system has to be able to store both of these instances in the same standardized object as well as combine them easily and take them in account when generating test sets for the other case.
\end{itemize}



\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{"../../../../Documents/TU Delft/Thesis 2/System Explanation/GeorgievObjectModel1"}
	\caption{Object model instantiating the set-up from \citet{georgiev2017low} after the TaskDatasets are created}
	\label{fig:georgievobjectmodel1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{"../../../../Documents/TU Delft/Thesis 2/System Explanation/GeorgievObjectModel2"}
	\caption{Object model instantiating the set-up from \citet{georgiev2017low} after the train and test sets are created along with the input objects for the Training}
	\label{fig:georgievobjectmodel2}
\end{figure}


In figures \ref{fig:georgievobjectmodel1} and \ref{fig:georgievobjectmodel2} two object models depicting instantiations of the set-up. In the first image is the situation where the TrainingSetCreator executed the TaskDataset generation method in each DataReader class. This illustrates how a Dataset which has a predefined test set is combined with the others which do not, while all of them have the same transformations in the ExtractionMethod object. The ExtractionMethod object then is composed of a LogbankSummaryExtraction, a PerCellScaling, a FramePreparation and a MedianWindowSizePreparationFitter object, accumulated in the LogbankSummaryExtractionMethod. A step-by-step explanation goes as follows:\\

The LogbankSummaryMethod extracts summaries of logbank filters as explained in the paper. PerCellScaling standardizes the data by calculating the mean and standard deviation of every separate cell aggregated from every feature matrix in the dataset. These are then used to achieve a per cell data distribution with a mean of 0 and a deviation of 1. FramePreparation cuts the matrices to frames of the same window size. This size is then decided by calculating the median matrix size in the dataset in the MedianWindowSizePreparationFitter. \\
 
In the next object model, it is demonstrated then what the situation is after the other datasets have their testDatasets split off and how these are accumulated in their concatenated train and test sets. These, along with a Results object and Multi-Task model form the input for training.\\


{\large \textbf{A Multi-task Learning Approach Based on Convolutional Neural Network for Acoustic Scene Classification}} \\

In this paper, by \citet{xu2019multi}, an Acoustic Event Detection (AED) task and a Acoustic Scene Classification (ASC) task are put together in the same multi task framework. Here a Convolutional Neural Network is used for classification with AED being an auxiliary task for improving the ASR task. The training data for the ASC task comes from the DCASE 2017 acoustic scene dataset while for the AED task it comes from the DCASE 2017 sound event dataset. The evaluation data comes from their respective predefined evaluation datasets. The evaluation metric is not simply accuracy which was the case in the previous work but Unweighted Average Recall.\\

Again, a number of lessons have been made from recreating this set-up:

\begin{itemize}
	\item This work has two predefined test sets, yet only one is used for evaluation as the focus is only one of the two tasks. Developers need to simply be able to control what goes in the training set and the test set separately, so training and evaluation can happen independently. On the same note, the framework should not only be made for combined datasets but be as receptive for single datasets.
	\item ASC and AED function differently. ASC is a task where one singular label is predicted for a whole sound file. AED detects whether or not a sound event is present at every time frame within a sound file. Multiple sound events - and thus multiple labels - can be given to a single feature matrix. These are respectively called multi-class tasks and multi-label tasks and the system thus needs the ability to combine both. They (often) require different loss calculation methods (in this case categorical cross entropy and binary cross entropy respectively), which thus should be decided in the system depending on the type of task. Following that the developer only should focus on one part of the pipeline at a time, the developer should be able to define the handling of the task when the task is defined (in other words in the Task object).
	\item Alongside handling the model output depending on the task definition, the model output head for each task itself should be adaptable on its definition. Multi-class tasks need only one output, which in this case is achieved by a SoftMax layer. Multi-label  tasks need multiple output labels which happens here through a sigmoid layer defining the activation value for each label. The dynamic output of models is important when different task combinations are made for the same system.
	\item There are numerous ways to evaluate the output of the system. Standard evaluation metrics should automatically be readily available, but the developer needs to be able to define their own required implementations easily. 
	\item The DCASE audio files are not mono audio files but stereo. This means that there are two time series in parallel for which individual feature matrices have to be made and appended. It's also possibly desired that the audio files are converted to mono files.
\end{itemize}

%TODO: Model instantiation and explain \\

{\large \textbf{Reporting the interaction between tasks}}\\ \label{Design:implementations:experiment}

In an effort to create a research like use case setting, which pushes some of the boundaries of the system as well as require the framework to be able to implement a more exploratory experimentation methodology, an exemplary experiment was devised and created. In this work, numerous datasets are extracted and combined in a multi-task model in order to investigate their resulting effect on each other and whether they are consistent when varying parts of the pipeline. The chosen datasets and tasks where Speaker Identification from the ASVspoof dataset \cite{wu2015asvspoof}, Sound Context Recognition from the Ambient acoustic context dataset \cite{park2020augmenting}, Acoustic Scene Classification from the DCASE 2017 Acoustic Scene Task dataset  and Acoustic Event Classification from the DCASE 2017 Acoustic Event Task dataset \cite{mesaros2017dcase}, Audio tagging from the FreeSound FSDKaggle 2018 dataset \cite{fonseca2017freesound}, Speech Emotion Recognition and Stress detection from the Ravdess dataset \cite{livingstone2012ravdess} and Keyword Detection from the Speech Commands dataset  \cite{warden2018speech}. \\

With this set-up the goal is not to be able to recreate an existing work, but to efficiently implement an exploratory experiment with varying parts. The idea is to run the pipeline for each singular dataset, two-by-two, three-by-three and then all combined in a singular function. What is of interest is whether patterns emerge when two tasks are learned together that are consistent. For this the patterns are thus noted from investigating the difference between single task and dual tasks. These patterns are then tracked when: a third task is added, all tasks are combined, a different extraction method is used and a different model is used. The extracted features chosen were the MelSpectrogram and the MFCC features. The two models were the ones from the previous two implementations which were a DNN and a CNN. \\

This requires from the framework that:
\begin{itemize}
	\item All the datasets can be extracted to TaskDatasets
	\item The extraction method can be efficiëntly switched out
	\item The model can be efficiëntly changed and instantiated with a different amount of heads
	\item A large number of different results can be clearly compared in multiple aspects
\end{itemize}

Earlier it was mentioned that the assumption was that a developer wants to walk away until the results are in. Therefore, the whole process and its variations are implemented in automated loops which weren't to be touched after initialization.\\

The resulting lessons are as follows:
\begin{itemize}
	\item The SpeechCommands dataset is an online dataset that can be accessed through a generator function from a library, but does not actually provide the sound files. Pre-processing and input insertion should not assume sound files are available. The generator function also means that before iteration, the raw dataset can't be accessed through indexing and its size can't be known (unless metadata is available). The framework should be able to build the taskdataset in this iterator-style setting with its builder functions.
	\item When switching out the extraction method the whole dataset has to be recalculated and the quick reading functionalities have to be able to differentiate between the used extraction methods.
	\item The model has to be able to switch as well adapt the number and type of heads depending on the current combination. This shows that there are unvarying parameters (like in this instance the amount of layers) and varying parameters (like the amount of heads). 
	\item The audio clip sizes vary significantly between datasets. The developer must be able to exactly control how they want to handle this. Transformations and their calculation functions therefore must be able to be executed both as if the combined datasets were one and for each one separate. To illustrate this: a developer can either choose to reform all feature matrices from all datasets to a singular size, but that might mean that too much information is lost for the more extreme matrix sizes. 
	\item Building on the previous point, one of the reasons that a developer might want to make their feature matrices of equal sizes is so that they can be batched together. Another possibility is that they want to keep their sizes intact internally for each dataset. Loading the data in the first case is covered by the standard PyTorch DataLoader behaviour as it can make random batches from each dataset it likes. However the second case where each batch is from a specific dataset is not, but should standardly be present as a way to ensure that different dataset combinations are possible out of the box.
	\item Observations in previous works like \citet{tonami2019joint} note that detection of events from AED combined with ASC did not improve if they were present in multiple scenes. Individual label detection performances must be able to be observed separately. 
	\item When trying to find working parameters as well as debug, a smaller validation set should be form-able, so that the whole dataset doesn't have to be involved.
	\item As not to waste unnecessary time in these sort of huge iterations of combinations, the dataset must be able to switch between a streaming context for larger (combined) datasets and loading the data in memory for smaller (combined) datasets.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
