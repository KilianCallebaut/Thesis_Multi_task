\chapter{Introduction}
TODO: Introduce the context of multi-task deep learning audio frameworks


\section{Example: Variating audio task interaction in Multi-Task Research}
TODO: Introduce original experiment set-ups as a basis for explaining what kind of multi-task development can be done, what the structure is and what it has to deal with

\section{Multi-Task Research}
TODO: Go further in depth about the general state of audio multi-task research and why this system is needed in that. Why is a speed up in development needed in the field?

Multi-task learning (MTL) is a machine learning paradigm where multiple different tasks are learned at the same time, exploiting underlying task relationships, to arrive at a shared representation. While the principle goal was to improve generalization accuracy of a machine learning system \citep{caruana1997multitask}, over the years multitask learning has found other uses, including speed of learning, improved intelligibility of learned models \citep{caruana1997multitask}, classification fairness \citep{oneto2019taking} and as a means to compress multiple parallel models \citep{georgiev2017heterogeneous}. This led to the paradigm finding its usage in multiple fields, including audio recognition.\\

The field of audio recognition is varied and ever expanding, due to a growing number of large public and non-publicly available datasets (e.g. AudioSet \citep{gemmeke2017audio}) each with their own variations like sources, lengths and subjects. The tasks in the field can roughly be divided into three categories: Speech recognition tasks, Environmental Sound recognition tasks and Music recognition tasks, along with tasks that combine multiple domains \citep{duan2014survey}. These domains inherently have a different structure from each other, which requires different processing and classification schemes. Speech for example, is inherently built up out of elementary phonemes that are internally dependent, the tasks linked to which have to deal with the exact differentiation and characterization of these, to varying degrees. Environmental sounds in contrast, do not have such substructures and cover a larger range of frequencies. Music then has its own stationary patterns like melody and rhythm \citep{boregowda2018environmental}. A general purpose audio classification system, dealing with real life audio, would have to deal with the presence of each of these types of audio though, regardless if its task is only in one of the domains.\\    

Usually, in order to achieve high performance, it is necessary to construct a focused detector, which targets a few classes per task. Only focusing on one set of targets with a fitting dataset however, ignores the wealth of information available in other task-specific datasets, as well as failing to leverage the fact that they might be calculating the same features, especially in the lower levels of the architecture \citep{tagliasacchi2020multi}. This does not only entail a possible waste of information (and thus performance) but also entails a waste of computational resources, as each task might not require its own dedicated model to achieve the same level of performance. Originally conventional methods like Gaussian Mixture Models (GMM) and State Vector Machines (SVM) were the main focus, but due to the impressive results in visual tasks deep learning architectures have seen a lot of attention.  The emergence of deep learning MTL set-ups is still fairly recent in audio recognition. While it has seen both successful \citep{tonami2019joint} applications and less successful \citep{sakti2016deep} when combining different tasks, very little is known about the exact circumstances when MTL works in audio recognition.\\

\section{Developing Deep Learning Multi-Task Set-ups}
TODO: Outline the steps in developing deep learning Multi-Task Set ups and how shortcuts can be made to speed up/improve the process. I.e. which problems have to be answered in the system. What developmental problems are you addressing?

Issues to face:

\textbf{Data Reading}
\begin{itemize}
	\item Developing valid input for loading and training for different datasets takes time and is error prone, while a lot of the processes are repetitive => DataReader to TaskDataset
	\item While developing and testing different set ups, intermediate parts (e.g. the feature extraction method, file reading method, resampling method) as well as additional parts (e.g. resampling) often have to be varied and replaced, which might be a complex and time consuming process depending on the amount of rewrites and datasets required => Easily interchangeable pipeline pieces
	\item Developing read/write functionalities per dataset is time consuming and potentially chaotic if done differently every time. Add to that the possibility of testing different set-ups for the same dataset which would require good file management. => Standardizing dataset read/write and automatic abstraction of reading when files are present
	
	\item loading in multiple datasets might be too memory intensive for a lot of systems 
	\item Running the code on a different system requires good datamanagement and changeable path locations
	\item While some datasets have predefined train/test sets, others do not, which would require different handling of both cases which might be time consuming and error prone (===> actually a consequence of standardizing in this way, i.e. engineering problem)
	\item Some Datasets can have multiple tasks on the same inputs (===> actually a consequence of standardizing in this way, i.e. engineering problem)
\end{itemize}

\textbf{Data Loading}
\begin{itemize}
	\item Each training procedure needs a train and test set, which for some datasets need to be created using k-fold validation set-ups and for some don't. When quickly trying to execute multiple set-ups this requires a lot of repetitive work. It's also error prone, as creating train/test sets the wrong way can cause data leaking and thus weaken the evaluation. (e.g. if the normalization is wrongfully calculated (-> the mean and stdev) on both the train and test set, the system will use information it shouldn't have and will perform unforeseenly worse on unseen data). => Abstraction to train/test set generation and handling
	\item Additional features like transforming or filtering the data again take up development time to specify for each separate dataset as well as can be a gruesome process to apply after the data is read into matrices. => abstraction to dataset functions that don't rely on knowledge of the matrix structures
\end{itemize}

\textbf{Training}
\begin{itemize}
	\item Combining datasets from tasks can be done in numerous ways, which can impact performance on training. => Allow multiple and extendible ways to combine tasks in the training batches
	\item In multi-task training, loss calculation is done by combining separate losses from tasks which can be done in numerous ways and might be interesting to explore => Allow multiple and extendible ways to combine losses in training
	\item In general for multi-task research, lots of parameters and parts should be varied => Allow replacability of each part in training, without jeopardizing the training function
	\item There are three types of task output structures in classification: binary, multi-class and multi-label outputs which each have to be handled uniquely while still being able to be combined => Abstraction of task type handling 
	\item Calculating, storing and visualizing results in an efficient way for comparison is crucial and can take up valuable development/debugging time => abstraction to calculating, storing and saving results that allows for easy comparison between runs
	\item Interrupted learning - the process of interrupting an ongoing training loop and restarting it later - requires good data management and saving of parameters to be loaded up again later, which is both error prone and time consuming => quick and easy way to restart an old run from a certain point
\end{itemize}

\textbf{Extra issues to be solved}
\begin{itemize}
	\item Figuring out the pipeline for multi-task deep learning set ups can be difficult, considering there are numerous types of and variations in multi-task learning schemes and not a lot of documentation on how to approach these
	\item Multi-task set-ups are most likely going to be compared to single task set-ups, meaning the code should already take this in account or handle the two cases separately
\end{itemize}

\section{Challenges}
TODO: Define the technological challenges in answering those problems. What problems/challenges do you face or have to take in account in developing such a system?

\section{Contributions}
TODO: Outline what new your thesis works contributes.

\section{Outline}
TODO: Summarize the rest of the thesis' structure