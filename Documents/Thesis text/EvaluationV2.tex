\chapter{Evaluation}

\section{Demonstrate Implementations}
% Implementations: Georgiev, 2nd, Chen and own
% Implementation: Test model on dataset
% 
% Do a lines of code comparison between the different components of the different implementations
% Componetns: Data Reading, Data Loading (Trainingsetcreator) and training
% Compare to implementation without?

% Make a table of lines of code comparisons
% Make comparisons with published results

% Make observation of reused lines of code, per section
% Make observations on what would be the problems from the original implementations if other things are required

\begin{table}[ht]
	\caption{Implementations LOC comparisons} % title of Table
	\centering % used for centering table
	\begin{tabular}%{p{0.2\textwidth}p{0.6\textwidth}p{0.2\textwidth}} % centered columns (4 columns)
		\hline\hline %inserts double horizontal lines
		Title & Total LOC  & Data Reading LOC & Data Loading LOC & Training LOC  \\ [0.5ex] % inserts table
		%heading
		\hline % inserts single horizontal line
		\citet{park2020augmenting} Without &  &  \\ \hline
		\citet{park2020augmenting} With &  &  \\ \hline
		\citet{georgiev2017low} Without &  &  \\ \hline
		\citet{georgiev2017low} With &  &  \\ \hline
		\citet{xu2019multi} Without &  &  \\ \hline
		\citet{xu2019multi} With &  &  \\ \hline
		Own Experiments &  &  \\ \hline
	\end{tabular}
	\label{table:LOC} % is used to refer this table in the text
\end{table}

\begin{table}[ht]
	\caption{Reported metric comparisons} % title of Table
	\centering % used for centering table
	\begin{tabular}%{p{0.2\textwidth}p{0.6\textwidth}p{0.2\textwidth}} % centered columns (4 columns)
		\hline\hline %inserts double horizontal lines
		Title & Metric & Reported & Implementation  \\ [0.5ex] % inserts table
		%heading
		\hline % inserts single horizontal line
		\citet{park2020augmenting} &  &  \\ \hline
		\citet{georgiev2017low} &  &  \\ \hline
		\citet{xu2019multi} &  &  \\ \hline
	\end{tabular}
	\label{table:metric} % is used to refer this table in the text
\end{table}

\begin{table}[ht]
	\caption{Reused code} % title of Table
	\centering % used for centering table
	\begin{tabular}%{p{0.2\textwidth}p{0.6\textwidth}p{0.2\textwidth}} % centered columns (4 columns)
		\hline\hline %inserts double horizontal lines
		Title & Metric & Reported & Implementation  \\ [0.5ex] % inserts table
		%heading
		\hline % inserts single horizontal line
		\citet{park2020augmenting} &  &  \\ \hline
		\citet{georgiev2017low} &  &  \\ \hline
		\citet{xu2019multi} &  &  \\ \hline
		Own Experiments
	\end{tabular}
	\label{table:metric} % is used to refer this table in the text
\end{table}

\section{Literature Evaluation}

%TODO: Dive in the literature and describe its implementation on the software, which components can be adjusted or nah

\section{Fulfilment of the Requirements}

\subsection{Non-functional Requirements}
\begin{itemize}
	\item \textbf{Modular:} 
	%	Demonstrate the replaceability in the code
	\item \textbf{Extendible:} 
	%	Extensibility is discussed in design section
	\item \textbf{Fast prototyping:} 
	% Demonstration implementations, comparison with implementation without
	\item \textbf{Cutting Double Work:} 
	% Demonstration of the research set-up
%	\item \textbf{Developmental Side-rails:}
%	%  
	\item \textbf{Robust:} 
%	Demonstration + Dive in the literature

\end{itemize}

\subsection{Funtional Requirements}
\subsubsection{Data Reading}
\begin{itemize}
	\item Standardizing Input - The TaskDataset object is developed for assuring that the data is valid throughout the rest of the process. Its extension of PyTorch's Dataset class ensures that it can be utilised by the PyTorch framework. The builder pattern allows the TaskDataset to be built incrementally and valid along the way, with each step including various validity checks.
	The exception where the TaskDataset can't check for validity is in terms of the input feature matrix size. The matrix sizes might not be compatible with the developed PyTorch Model. The responsibility for this is up to the developer.
	% How do we know this works?
	% What validity checks?
	\item Handling dataset differences - The DataReader class is an abstract class that the developer must extend to deal with the peculiarities of navigating each dataset structure to extract the correct information. This corresponds to it being a white box hot-spot. Predefined train/test splits can be stored through the HoldTaskDataset structure and pre-split audio segments can be kept together by defining the grouping. 
	% Refer to the implementations for differences
	\item Scalable preprocessing - Preprocessing audio signals and preprocessing feature matrices happen in different places, as TaskDatasets should only contain valid input instances at any point. Preprocessing signals can utilise an (optional) function from the DataReader class with parameters that are received when the TaskDataset is extracted. Reusing the method can thus hand developers easy replicability of the signal preprocessing. These can be further scaled by using the TrainingSetCreator. In this class, any preprocessing or transformation can be added 'on the fly'. This means that if a functionality (e.g. resampling) is added, any previous 
	% What preprocessing do we have for both signals and feature matrices
	\item File storage abstraction: There are handles on the TaskDataset which can be called to store, load or check the TaskDataset to or from files, which are specific for the currently used extraction method and task.
	\item Quick Reading: The DataReader automatically checks if there is a stored TaskDataset available for the given extraction\_method and task and loads it if so.
	\item Create multiple input objects from the same dataset: (DEMONSTRATE) The framework is open ended in how the TaskDataset object is extracted from the data and allows extra parameters for the DataReader to be given at initialization. TaskDatasets are stored using the ExtractionMethod object's name and the (main) Task's name, so for every new variation of these will be automatically linked to different files.
	\item Tasks and datasets are a many to many relationship: Tasks can be present in multiple datasets. The tasks need to have the same name, output labels and classification type in order to be seen as the same. When combined in the ConcatTaskDataset, the target vectors will automatically be placed in the same positions, which will make them be seen as the same task in the training function. Datasets can have multiple tasks to an unlimited degree in its list of extra tasks, which always combines them with a list of targets of the same amount of input instances. 
\end{itemize}
\subsubsection{Data Loading}
\begin{itemize}
	\item Combining datasets: Extended the ConcatTaskDataset, extra functions for the batching
	\item Not requiring the combined datasets in memory:  Index mode implemented which forms a streaming context for the data to be stored and read from disk.
	\item Train and test set generation: They implemented through the HoldTaskDataset
	\item Transforming data: They implemented through handles on the TaskDataset which call upon the ExtractionMethod
	\item Filtering data: Handle on the TaskDataset
	\item Reusing data: It implemented, reusability is possible without reloading to different degrees
	\item Batching multiple tasks: It implemented lol
	\item Replicability: It implemented through storing the different checkpoints (which can be recreated by reloading the Results object), the calculation data in extractionmethods and 
	\item Scalable Manipulation: In the TrainingSetCreator, manipulations can be added for one specified dataset or all at once.
\end{itemize}
\subsubsection{Training}
\begin{itemize}
	\item Predicting multiple tasks: Each dataset can have multiple tasks linked to their targets. There are automatic filters for the output to isolate the task specific predictions. 
	\item Task specific output handling: Handling of the task functions, like loss calculation and decision making of the eventual classes from the probabilities are stored in the Task objects 
	\item Loss calculation specifiable: The calculation of the Loss of each task is definable in the Task object. However, currently, losses are always tied to tasks, which have target labels. This is not always the case, as in the research \citep{tagliasacchi2020multi} \citep{wu2020domain}, losses have also been calculated based on cost functions from internal model parameters. Since these loss functions are not linked to datasets, but to the models themselves, for which the framework does not offer modules which can be used in the training function for specific handling, it is up to the developer to implement these in the Training\_Utils object.
	\item Loss combination specifiable: Implemented in the Training\_Utils object
	\item Metric calculation, storage and visualization: Gives predictions, true labels and losses to the Results object which calculates the metrics, stores them and writes them to tensorboard where they can easily be compared to other results
	\item Interrupted Learning: (DEMONSTRATE) Implemented by recreating the Results object and starting the training loop from the given epoch.
	\item Separate evaluation: (DEMONSTRATE) The evaluation function is separate from the training loop. Training parameters for transformations and such can be reloaded from the stored extraction\_method object as well as the model parameters at every epoch in the training function. 
	\item Direct comparison of different runs: Every run has a unique name and TensorBoard has the ability to place the results from different files side-by-side
	\item Variable training paradigms: In this state, the only training paradigm available is Gradient Descent. Implementing a different paradigm requires foregoing the current training loop implementation.
\end{itemize}